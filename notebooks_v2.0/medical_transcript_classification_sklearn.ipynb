{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Medical transcript classification [sklearn]\n",
    "* Multiclass classification of medical transcript.\n",
    "* Reference notebook: <https://www.kaggle.com/code/leekahwin/text-classification-using-n-gram-0-8-f1/notebook>\n",
    "* Dataset: <https://www.kaggle.com/code/leekahwin/text-classification-using-n-gram-0-8-f1/input>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install necessary dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from typing import Iterable\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import giskard\n",
    "from giskard import Dataset, Model, GiskardClient\n",
    "from giskard.client.giskard_client import GiskardError"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download NLTK stopwords corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download list of english stopwords.\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants.\n",
    "LABELS_LIST = [\n",
    "    'Neurosurgery',\n",
    "    'ENT - Otolaryngology',\n",
    "    'Discharge Summary',\n",
    "    'General Medicine',\n",
    "    'Gastroenterology',\n",
    "    'Neurology',\n",
    "    'SOAP / Chart / Progress Notes',\n",
    "    'Obstetrics / Gynecology',\n",
    "    'Urology'\n",
    "]\n",
    "\n",
    "TEXT_COLUMN_NAME = \"transcription\"\n",
    "TARGET_COLUMN_NAME = \"medical_specialty\"\n",
    "\n",
    "RANDOM_SEED = 8888\n",
    "\n",
    "# Giskard creds.\n",
    "GISKARD_URL = \"http://localhost:9000\"\n",
    "GISKARD_TOKEN = \"\"\n",
    "GISKARD_PROJECT_KEY = \"medical_transcript_classification\"\n",
    "\n",
    "# Paths.\n",
    "PATH_DATA = os.path.join(\".\", \"datasets\", \"medical_transcript_classification_dataset\", \"mtsamples.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Load and initially preprocess data.\"\"\"\n",
    "    df = pd.read_csv(PATH_DATA)\n",
    "\n",
    "    # Drop useless columns.\n",
    "    df = df.drop(columns=['Unnamed: 0', \"description\", \"sample_name\", \"keywords\"])\n",
    "\n",
    "    # Trim text.\n",
    "    df = df.apply(lambda x: x.str.strip())\n",
    "\n",
    "    # Filter samples by label.\n",
    "    df = df[df[TARGET_COLUMN_NAME].isin(LABELS_LIST)]\n",
    "\n",
    "    # Drop rows with no transcript.\n",
    "    df = df[df[TEXT_COLUMN_NAME].notna()]\n",
    "\n",
    "    return df\n",
    "\n",
    "transcript_df = load_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(transcript_df[[TEXT_COLUMN_NAME]], transcript_df[TARGET_COLUMN_NAME],\n",
    "                                                    random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap dataset with giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data = pd.concat([X_test, y_test], axis=1)\n",
    "wrapped_data = Dataset(raw_data,\n",
    "                       name=\"medical_transcript_dataset\",\n",
    "                       target=TARGET_COLUMN_NAME,\n",
    "                       column_types={TEXT_COLUMN_NAME: \"text\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define preprocessing steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess text.\"\"\"\n",
    "    # Lower.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove punctuation.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Tokenize.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: x.split())\n",
    "\n",
    "    # Stem.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Remove stop-words.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n",
    "\n",
    "    return df\n",
    "\n",
    "text_preprocessor = FunctionTransformer(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adapt_vectorizer_input(df: pd.DataFrame) -> Iterable:\n",
    "    \"\"\"Adapt input for the vectorizers.\n",
    "\n",
    "    The problem is that vectorizers accept iterable, not DataFrame, but Series. Thus, we need to ravel dataframe with text have input single dimension.\n",
    "    Issue reference: https://stackoverflow.com/questions/50665240/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-1-3185\"\"\"\n",
    "\n",
    "    df = df.iloc[:, 0]\n",
    "    return df\n",
    "\n",
    "vectorizer_input_adapter = FunctionTransformer(adapt_vectorizer_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define final pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    (\"text_preprocessor\", text_preprocessor),\n",
    "    (\"vectorizer_input_adapter\", vectorizer_input_adapter),\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 1))),\n",
    "    (\"estimator\", RandomForestClassifier(random_state=RANDOM_SEED))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit and test estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eafbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:12.195472Z",
     "iopub.status.busy": "2022-09-14T15:45:12.194744Z",
     "iopub.status.idle": "2022-09-14T15:45:12.218246Z",
     "shell.execute_reply": "2022-09-14T15:45:12.216597Z"
    },
    "papermill": {
     "duration": 0.053495,
     "end_time": "2022-09-14T15:45:12.220712",
     "exception": false,
     "start_time": "2022-09-14T15:45:12.167217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define prediction function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediction_function(df: pd.DataFrame) -> np.ndarray:\n",
    "    return pipeline.predict_proba(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap model with giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model = Model(prediction_function,\n",
    "                      model_type=\"classification\",\n",
    "                      name=\"medical_transcript_classification\",\n",
    "                      feature_names=[TEXT_COLUMN_NAME],\n",
    "                      classification_labels=pipeline.classes_)\n",
    "\n",
    "# Validate wrapped model and data.\n",
    "print(classification_report(y_test, pipeline.classes_[wrapped_model.predict(wrapped_data).raw_prediction]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform model scan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scanning_results = giskard.scan(wrapped_model, wrapped_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(scanning_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload model and dataset to the giskard UI platform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Init giskard client.\n",
    "client = GiskardClient(GISKARD_URL, GISKARD_TOKEN)\n",
    "\n",
    "# Create or fetch a project by its key.\n",
    "try:\n",
    "    project = client.create_project(GISKARD_PROJECT_KEY,\n",
    "                                    name=\"MEDICAL_TRANSCRIPTS_CLASSIFICATION\",\n",
    "                                    description=\"Multiclass classification of the diagnosis based on medical transcript text.\")\n",
    "except GiskardError as e:\n",
    "    print(f\"Project with key {GISKARD_PROJECT_KEY} already exists. Trying to get it.\")\n",
    "    project = client.get_project(GISKARD_PROJECT_KEY)\n",
    "\n",
    "# Upload model and dataset.\n",
    "model_id = wrapped_model.upload(client, GISKARD_PROJECT_KEY)\n",
    "dataset_id = wrapped_data.upload(client, GISKARD_PROJECT_KEY)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1295.42731,
   "end_time": "2022-09-14T15:45:13.308486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-14T15:23:37.881176",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
