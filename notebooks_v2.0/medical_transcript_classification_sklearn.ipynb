{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Medical transcript classification [sklearn]\n",
    "* Multiclass classification of medical transcript.\n",
    "* Reference notebook: <https://www.kaggle.com/code/leekahwin/text-classification-using-n-gram-0-8-f1/notebook>\n",
    "* Dataset: <https://www.kaggle.com/code/leekahwin/text-classification-using-n-gram-0-8-f1/input>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quickstart\n",
    "\n",
    "By running this notebook, you’ll create a whole test suite in a few lines of code. The model used here is a Random Forest classification model with the medical transcript dataset. Feel free to use your own model (tabular, text, or LLM).\n",
    "\n",
    "You’ll learn how to:\n",
    "* Detect vulnerabilities by scanning the model\n",
    "* Generate a test suite with domain-specific tests\n",
    "* Customize your test suite by loading a test from the Giskard catalog\n",
    "* Upload your model to the Giskard server to:\n",
    "* Compare models to decide which one to promote\n",
    "* Debug your tests to diagnose issues\n",
    "* Share your results and collect business feedback from your team"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install necessary dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from typing import Iterable\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from urllib.request import urlretrieve\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import giskard\n",
    "from giskard import Dataset, Model, GiskardClient, testing\n",
    "from giskard.client.giskard_client import GiskardError"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download NLTK stopwords corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download list of english stopwords.\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants.\n",
    "LABELS_LIST = [\n",
    "    'Neurosurgery',\n",
    "    'ENT - Otolaryngology',\n",
    "    'Discharge Summary',\n",
    "    'General Medicine',\n",
    "    'Gastroenterology',\n",
    "    'Neurology',\n",
    "    'SOAP / Chart / Progress Notes',\n",
    "    'Obstetrics / Gynecology',\n",
    "    'Urology'\n",
    "]\n",
    "\n",
    "TEXT_COLUMN_NAME = \"transcription\"\n",
    "TARGET_COLUMN_NAME = \"medical_specialty\"\n",
    "\n",
    "RANDOM_SEED = 8888\n",
    "\n",
    "# Data.\n",
    "DATA_URL = os.path.join(\"ftp://sys.giskard.ai\", \"pub\", \"unit_test_resources\", \"medical_transcript_classification_dataset\", \"mtsamples.csv\")\n",
    "DATA_PATH = Path.home() / \".giskard\" / \"medical_transcript_classification_dataset\" / \"mtsamples.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fetch_from_ftp(url: str, file: Path) -> None:\n",
    "    \"\"\"Helper to fetch data from the FTP server.\"\"\"\n",
    "    if not file.parent.exists():\n",
    "        file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not file.exists():\n",
    "        print(f\"Downloading data from {url}\")\n",
    "        urlretrieve(url, file)\n",
    "\n",
    "    print(f\"Data was loaded!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Load and initially preprocess data.\"\"\"\n",
    "    fetch_from_ftp(DATA_URL, DATA_PATH)\n",
    "\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    # Drop useless columns.\n",
    "    df = df.drop(columns=['Unnamed: 0', \"description\", \"sample_name\", \"keywords\"])\n",
    "\n",
    "    # Trim text.\n",
    "    df = df.apply(lambda x: x.str.strip())\n",
    "\n",
    "    # Filter samples by label.\n",
    "    df = df[df[TARGET_COLUMN_NAME].isin(LABELS_LIST)]\n",
    "\n",
    "    # Drop rows with no transcript.\n",
    "    df = df[df[TEXT_COLUMN_NAME].notna()]\n",
    "\n",
    "    return df\n",
    "\n",
    "transcript_df = load_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(transcript_df[[TEXT_COLUMN_NAME]],\n",
    "                                                    transcript_df[TARGET_COLUMN_NAME],\n",
    "                                                    random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap dataset with giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data = pd.concat([X_test, y_test], axis=1)\n",
    "wrapped_data = Dataset(raw_data,\n",
    "                       name=\"medical_transcript_dataset\",\n",
    "                       target=TARGET_COLUMN_NAME,\n",
    "                       column_types={TEXT_COLUMN_NAME: \"text\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define preprocessing steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess text.\"\"\"\n",
    "    # Lower.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove punctuation.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Tokenize.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: x.split())\n",
    "\n",
    "    # Stem.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Remove stop-words.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n",
    "\n",
    "    return df\n",
    "\n",
    "text_preprocessor = FunctionTransformer(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adapt_vectorizer_input(df: pd.DataFrame) -> Iterable:\n",
    "    \"\"\"Adapt input for the vectorizers.\n",
    "\n",
    "    The problem is that vectorizers accept iterable, not DataFrame, but Series. Thus, we need to ravel dataframe with text have input single dimension.\n",
    "    Issue reference: https://stackoverflow.com/questions/50665240/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-1-3185\"\"\"\n",
    "\n",
    "    df = df.iloc[:, 0]\n",
    "    return df\n",
    "\n",
    "vectorizer_input_adapter = FunctionTransformer(adapt_vectorizer_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define final pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    (\"text_preprocessor\", text_preprocessor),\n",
    "    (\"vectorizer_input_adapter\", vectorizer_input_adapter),\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 1))),\n",
    "    (\"estimator\", RandomForestClassifier(random_state=RANDOM_SEED))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit and test estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eafbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:12.195472Z",
     "iopub.status.busy": "2022-09-14T15:45:12.194744Z",
     "iopub.status.idle": "2022-09-14T15:45:12.218246Z",
     "shell.execute_reply": "2022-09-14T15:45:12.216597Z"
    },
    "papermill": {
     "duration": 0.053495,
     "end_time": "2022-09-14T15:45:12.220712",
     "exception": false,
     "start_time": "2022-09-14T15:45:12.167217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap model with giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model = Model(pipeline.predict_proba,\n",
    "                      model_type=\"classification\",\n",
    "                      name=\"medical_transcript_classification\",\n",
    "                      feature_names=[TEXT_COLUMN_NAME],\n",
    "                      classification_labels=pipeline.classes_)\n",
    "\n",
    "# Validate wrapped model and data.\n",
    "print(classification_report(y_test, pipeline.classes_[wrapped_model.predict(wrapped_data).raw_prediction]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scan your model to find vulnerabilities\n",
    "With the Giskard scan feature, you can detect vulnerabilities in your model, including performance biases, unrobustness, data leakage, stochasticity, underconfidence, ethical issues, and more. For detailed information about the scan feature, please refer to our scan documentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = giskard.scan(wrapped_model, wrapped_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate a test suite from the Scan\n",
    "The objects produced by the scan can be used as fixtures to generate a test suite that integrate domain-specific issues. To create custom tests, refer to the Test your ML Model page."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_suite = results.generate_test_suite(\"My first test suite\")\n",
    "test_suite.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customize your suite by loading objects from the Giskard catalog\n",
    "\n",
    "The Giskard open source catalog will enable to load:\n",
    "* Tests such as metamorphic, performance, prediction & data drift, statistical tests, etc\n",
    "* Slicing functions such as detectors of toxicity, hate, emotion, etc\n",
    "* Transformation functions such as generators of typos, paraphrase, style tune, etc\n",
    "\n",
    "For demo purposes, we will load a simple unit test (test_f1) that checks if the test F1 score is above the given threshold. For more examples of tests and functions, refer to the Giskard catalog."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_suite.add_test(testing.test_f1(model=wrapped_model, dataset=wrapped_data, threshold=0.7)).run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload your suite to the Giskard server\n",
    "\n",
    "Upload your suite to the Giskard server to:\n",
    "* Compare models to decide which model to promote\n",
    "* Debug your tests to diagnose the issues\n",
    "* Create more domain-specific tests that are integrating business feedback\n",
    "* Share your results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Uploading the test suite will automatically save the model, dataset, tests, slicing & transformation functions inside the Giskard UI server\n",
    "# Create a Giskard client after having install the Giskard server (see documentation)\n",
    "token = \"API_TOKEN\"  # Find it in Settings in the Giskard server\n",
    "\n",
    "client = GiskardClient(\n",
    "    url=\"http://localhost:19000\",  # URL of your Giskard instance\n",
    "    token=token\n",
    ")\n",
    "\n",
    "my_project = client.create_project(\"my_project\", \"PROJECT_NAME\", \"DESCRIPTION\")\n",
    "\n",
    "# Upload to the current project ✉️\n",
    "test_suite.upload(client, \"my_project\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1295.42731,
   "end_time": "2022-09-14T15:45:13.308486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-14T15:23:37.881176",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
