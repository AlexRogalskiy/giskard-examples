{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# IEEE Fraud detection adversarial validation [sklearn, lgbm]\n",
    "IEEE Fraud detection train/test data binary classification task.\n",
    "Reference: <https://www.kaggle.com/code/jtrotman/ieee-fraud-adversarial-lgb-split-points/notebook>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pandas.api.types import union_categoricals\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import giskard\n",
    "from giskard import GiskardClient\n",
    "from giskard import Dataset, Model\n",
    "from giskard.client.giskard_client import GiskardError"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants.\n",
    "TARGET_COLUMN = 'isTest'\n",
    "IDX_LABEL = 'TransactionID'\n",
    "\n",
    "# Paths.\n",
    "DATA_URL = os.path.join(\"ftp://sys.giskard.ai\", \"pub\", \"unit_test_resources\", \"fraud_detection_classification_dataset\", \"{}\")\n",
    "DATA_PATH = Path.home() / \".giskard\" / \"fraud_detection_classification_dataset\"\n",
    "\n",
    "# Giskard creds.\n",
    "GISKARD_URL = \"http://localhost:9000\"\n",
    "GISKARD_TOKEN = \"\"\n",
    "GISKARD_PROJECT_KEY = \"fraud_detection_adversarial_validation\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading and preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fetch_from_ftp(url: str, file: Path) -> None:\n",
    "    \"\"\"Helper to fetch data from the FTP server.\"\"\"\n",
    "    if not file.parent.exists():\n",
    "        file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not file.exists():\n",
    "        print(f\"Downloading data from {url}\")\n",
    "        urlretrieve(url, file)\n",
    "\n",
    "    print(f\"Data was loaded!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fetch_dataset():\n",
    "    files_to_fetch = [\"train_transaction.csv\", \"train_identity.csv\", \"test_transaction.csv\", \"test_identity.csv\"]\n",
    "    for file_name in files_to_fetch:\n",
    "        fetch_from_ftp(DATA_URL.format(file_name), DATA_PATH / file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define data-types of transactions features.\n",
    "DATA_TYPES_TRANSACTION = {\n",
    "    'TransactionID': 'int32',\n",
    "    'isFraud': 'int8',\n",
    "    'TransactionDT': 'int32',\n",
    "    'TransactionAmt': 'float32',\n",
    "    'ProductCD': 'category',\n",
    "    'card1': 'int16',\n",
    "    'card2': 'float32',\n",
    "    'card3': 'float32',\n",
    "    'card4': 'category',\n",
    "    'card5': 'float32',\n",
    "    'card6': 'category',\n",
    "    'addr1': 'float32',\n",
    "    'addr2': 'float32',\n",
    "    'dist1': 'float32',\n",
    "    'dist2': 'float32',\n",
    "    'P_emaildomain': 'category',\n",
    "    'R_emaildomain': 'category',\n",
    "}\n",
    "\n",
    "C_COLS = [f'C{i}' for i in range(1, 15)]\n",
    "D_COLS = [f'D{i}' for i in range(1, 16)]\n",
    "M_COLS = [f'M{i}' for i in range(1, 10)]\n",
    "V_COLS = [f'V{i}' for i in range(1, 340)]\n",
    "\n",
    "DATA_TYPES_TRANSACTION.update((c, 'float32') for c in C_COLS)\n",
    "DATA_TYPES_TRANSACTION.update((c, 'float32') for c in D_COLS)\n",
    "DATA_TYPES_TRANSACTION.update((c, 'float32') for c in V_COLS)\n",
    "DATA_TYPES_TRANSACTION.update((c, 'category') for c in M_COLS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define datatypes of identity features.\n",
    "DATA_TYPES_ID = {\n",
    "    'TransactionID': 'int32',\n",
    "    'DeviceType': 'category',\n",
    "    'DeviceInfo': 'category',\n",
    "}\n",
    "\n",
    "ID_COLS = [f'id_{i:02d}' for i in range(1, 39)]\n",
    "ID_CATS = [\n",
    "    'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30',\n",
    "    'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'\n",
    "]\n",
    "\n",
    "DATA_TYPES_ID.update(((c, 'float32') for c in ID_COLS))\n",
    "DATA_TYPES_ID.update(((c, 'category') for c in ID_CATS))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define list of all categorical features.\n",
    "CATEGORICALS = [f_name for (f_name, f_type) in dict(DATA_TYPES_TRANSACTION, **DATA_TYPES_ID).items() if f_type == \"category\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_set(_type):\n",
    "    \"\"\"Read both transactions and identity data.\"\"\"\n",
    "    print(f\"Reading transactions data...\")\n",
    "    _df = pd.read_csv(os.path.join(DATA_PATH, f'{_type}_transaction.csv'),\n",
    "                      index_col=IDX_LABEL, dtype=DATA_TYPES_TRANSACTION, nrows=250)\n",
    "\n",
    "    print(f\"Reading identity data...\")\n",
    "    _df = _df.join(pd.read_csv(os.path.join(DATA_PATH, f'{_type}_identity.csv'),\n",
    "                               index_col=IDX_LABEL, dtype=DATA_TYPES_ID))\n",
    "    return _df\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"Read whole data.\"\"\"\n",
    "\n",
    "    fetch_dataset()\n",
    "\n",
    "    print(f\"Reading train data...\")\n",
    "    train_set = read_set('train')\n",
    "\n",
    "    print(f\"Reading test data...\")\n",
    "    test_set = read_set('test')\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def preprocess_dataset(train_set, test_set):\n",
    "    \"\"\"Unite train and test into common dataframe.\"\"\"\n",
    "    # Create a new target column and remove a former one from the train data.\n",
    "    print(\"Start data preprocessing...\")\n",
    "    train_set.pop('isFraud')\n",
    "    train_set['isTest'] = 0\n",
    "    test_set['isTest'] = 1\n",
    "\n",
    "    # Preprocess categorical features.\n",
    "    n_train = train_set.shape[0]\n",
    "    for c in train_set.columns:\n",
    "        s = train_set[c]\n",
    "        if hasattr(s, 'cat'):\n",
    "            u = union_categoricals([train_set[c], test_set[c]], sort_categories=True)\n",
    "            train_set[c] = u[:n_train]\n",
    "            test_set[c] = u[n_train:]\n",
    "\n",
    "    # Unite train and test data.\n",
    "    united = pd.concat([train_set, test_set])\n",
    "\n",
    "    # Add additional features.\n",
    "    united['TimeInDay'] = united.TransactionDT % 86400\n",
    "    united['Cents'] = united.TransactionAmt % 1\n",
    "\n",
    "    # Remove useless columns.\n",
    "    united.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "    print(f\"Dataset merged and preprocessed! Resulted shape: {united.shape}\")\n",
    "\n",
    "    return united"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "united_dataset = preprocess_dataset(*read_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(united_dataset.drop(TARGET_COLUMN, axis=1), united_dataset[TARGET_COLUMN], test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_dataset = pd.concat([X_test, y_test], axis=1)\n",
    "wrapped_dataset = Dataset(raw_dataset,\n",
    "                          name=\"fraud_detection_adversarial_dataset\",\n",
    "                          target=TARGET_COLUMN,\n",
    "                          cat_columns=CATEGORICALS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define parameters of an estimator.\n",
    "ESTIMATOR_PARAMS = {\n",
    "    'num_leaves': 64,\n",
    "    'objective': 'binary',\n",
    "    'min_data_in_leaf': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 1,\n",
    "    'max_cat_to_onehot': 128,\n",
    "    'metric': 'auc',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'subsample_for_bin': united_dataset.shape[0]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "estimator = LGBMClassifier(**ESTIMATOR_PARAMS)\n",
    "estimator.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_metric = roc_auc_score(y_train, estimator.predict_proba(X_train)[:, 1].T)\n",
    "test_metric = roc_auc_score(y_test, estimator.predict_proba(X_test)[:, 1].T)\n",
    "\n",
    "print(f\"Train ROC-AUC score: {train_metric}\")\n",
    "print(f\"Test ROC-AUC score: {test_metric}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediction_function(df: pd.DataFrame) -> np.ndarray:\n",
    "    return estimator.predict_proba(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model = Model(prediction_function,\n",
    "                      model_type=\"classification\",\n",
    "                      name=\"train_test_data_classifier\",\n",
    "                      feature_names=X_train.columns,\n",
    "                      classification_threshold=0.5,\n",
    "                      classification_labels=[0, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Validate wrapped model.\n",
    "wrapped_test_metric = roc_auc_score(y_test, wrapped_model.predict(wrapped_dataset).raw[:, 1].T)\n",
    "print(f\"Wrapped Test ROC-AUC score: {wrapped_test_metric}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scan model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scanning_results = giskard.scan(wrapped_model, wrapped_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(scanning_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload model and dataset to the Giskard platform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Init new giskard client.\n",
    "client = GiskardClient(GISKARD_URL, GISKARD_TOKEN)\n",
    "\n",
    "# Create or fetch a project by its key.\n",
    "try:\n",
    "    project = client.create_project(GISKARD_PROJECT_KEY,\n",
    "                                    name=\"FRAUD_DETECTION_ADVERSARIAL_VALIDATION\",\n",
    "                                    description=\"Perform classification of data, drawing from train or test sample, to define problematic features.\")\n",
    "except GiskardError as e:\n",
    "    print(f\"Project with key {GISKARD_PROJECT_KEY} already exists. Trying to get it.\")\n",
    "    project = client.get_project(GISKARD_PROJECT_KEY)\n",
    "\n",
    "# Upload the model and the dataset.\n",
    "model_id = wrapped_model.upload(client, GISKARD_PROJECT_KEY)\n",
    "dataset_id = wrapped_dataset.upload(client, GISKARD_PROJECT_KEY)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
